{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = \"\"\"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\"\"\"\n",
    "sample2 = \"\"\"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\"\"\"\n",
    "sample3 = \"\"\"We are always looking for opportunities for you to earn more points, which is why we also give you a selection of Special Offers. These Special Offers are opportunities to earn bonus points on top of the regular points you earn every time you purchase a participating brand. No need to pre-select these offers, we'll give you the points whether or not you knew about the offer. We just think it is easier that way.\"\"\"\n",
    "\n",
    "sample1 = sample1.lower()\n",
    "sample2 = sample2.lower()\n",
    "sample3 = sample3.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(s):\n",
    "    punctuations = \".,:;\\\\!?\\\"\"\n",
    "    contractions = {\"n't\" : \" not\", \"'ve\" : \" have\", \"'ll\" : \" will\"}\n",
    "    for i in punctuations:\n",
    "        s = s.replace(i, \"\")\n",
    "    \n",
    "    for j in contractions.keys():\n",
    "        s = s.replace(j, contractions[j])\n",
    "    \n",
    "    return s\n",
    "\n",
    "def words_to_dict(s):\n",
    "    wd = {}\n",
    "    list_s = s.split(\" \")\n",
    "    for i in list_s:\n",
    "        if(len(i)>2):\n",
    "            if(i in wd.keys()):\n",
    "                wd[i] += 1\n",
    "            else:\n",
    "                wd[i] = 1\n",
    "    return wd\n",
    "\n",
    "def doing_dats(vocab, w):\n",
    "    temp_list = []\n",
    "    for i1 in range(len(vocab.items())):\n",
    "            temp_list.append(0)\n",
    "\n",
    "    for item in vocab.keys():\n",
    "        if(item in w.keys()):\n",
    "            temp_list[vocab[item]] = 1\n",
    "    return temp_list\n",
    "\n",
    "def process_sample(sample1, sample2):\n",
    "    w1 = words_to_dict(clean_data(sample1))\n",
    "    w2 = words_to_dict(clean_data(sample2))\n",
    "    all_words = list(w1.keys())\n",
    "    all_words.extend(list(w2.keys()))\n",
    "    all_words = list(set(all_words))\n",
    "    all_words.sort()\n",
    "    #n = len(all_words)\n",
    "    #return (similarity_check1(w1,w2,n))\n",
    "    return (w1,w2,all_words)\n",
    "\n",
    "# Jaccard Similarity\n",
    "def similarity_check1(dict1, dict2, n):\n",
    "    common1 = len(list(set(dict1) & set(dict2)))\n",
    "    similarity1 = common1/n\n",
    "    return(round(similarity1,5))\n",
    "\n",
    "# Cosine Similarity\n",
    "def similarity_check2(main_list, w1, w2):\n",
    "    numer = 0\n",
    "    for p2 in range(len(main_list[0])):\n",
    "        numer += main_list[0][p2] * main_list[1][p2]\n",
    "\n",
    "    den1 = 0\n",
    "    for p3 in range(len(main_list[0])):\n",
    "        den1 += main_list[0][p3] ** 2 #main_list[0][p3]\n",
    "    den2 = 0\n",
    "    for p4 in range(len(main_list[1])):\n",
    "        den2 += main_list[1][p4] ** 2 #main_list[1][p4]\n",
    "        \n",
    "    denr = (den1*den2) ** (1/2)\n",
    "    return(round((numer/denr),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [sample1, sample2, sample3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_similarity1(s):\n",
    "    results = []\n",
    "    for i in range(len(s)-1):\n",
    "        for j in range(i+1,len(s)):\n",
    "            (d1, d2, all_words) = process_sample(s[i], s[j])\n",
    "            n = len(all_words)\n",
    "            result = similarity_check1(d1, d2, n)\n",
    "            print(\"Similarity Measure 1 between Sample \"+str(i+1)+\" and \"+str(j+1)+\"     ---> \"+str(result))\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_similarity2(s):\n",
    "    results = []\n",
    "    for i in range(len(s)-1):\n",
    "        for j in range(i+1,len(s)):\n",
    "            (d1, d2, all_words) = process_sample(s[i], s[j])\n",
    "\n",
    "            vocab = {}\n",
    "            for ind, word in enumerate(all_words):\n",
    "                vocab[word] = ind\n",
    "            main_list = []\n",
    "            list1 = doing_dats(vocab, d1)\n",
    "            list2 = doing_dats(vocab, d2)\n",
    "            main_list.append(list1)\n",
    "            main_list.append(list2)\n",
    "            result = similarity_check2(main_list, d1, d2)\n",
    "            print(\"Similarity Measure 2 between Sample \"+str(i+1)+\" and \"+str(j+1)+\"     ---> \"+str(result))\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Measure 1 between Sample 1 and 2     ---> 0.69565\n",
      "Similarity Measure 1 between Sample 1 and 3     ---> 0.16667\n",
      "Similarity Measure 1 between Sample 2 and 3     ---> 0.14493\n",
      "-------------------------------------------------------------\n",
      "Similarity Measure 2 between Sample 1 and 2     ---> 0.82078\n",
      "Similarity Measure 2 between Sample 1 and 3     ---> 0.28574\n",
      "Similarity Measure 2 between Sample 2 and 3     ---> 0.25318\n",
      "-------------------------------------------------------------\n",
      "Final Similarity Measure between Sample 1 and 2 ---> 0.758215\n",
      "Final Similarity Measure between Sample 1 and 3 ---> 0.758215\n",
      "Final Similarity Measure between Sample 2 and 3 ---> 0.226205\n"
     ]
    }
   ],
   "source": [
    "r1 = exec_similarity1(s)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "r2 = exec_similarity2(s)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "for i in range(len(r1)-1):\n",
    "    for j in range(i+1,len(r1)):\n",
    "        print(\"Final Similarity Measure between Sample \"+str(i+1)+\" and \"+str(j+1)+\" ---> \"+str((r1[i]+r2[i])/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW CODE DID NOT HAVE GOOD OUTPUTS ON OUR CURRENT SAMPLE - NOT PART OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "s1 = clean_data(sample1); s2 = clean_data(sample2); s3 = clean_data(sample3);\n",
    "\n",
    "s1_list = s1.split(\" \")\n",
    "s2_list = s2.split(\" \")\n",
    "s3_list = s2.split(\" \")\n",
    "df1 = {}; df2 = {}; df3 = {};\n",
    "\n",
    "# FUNCTIONS START\n",
    "def doc_len(doc_list):\n",
    "    temp = [x for x in doc_list if len(x)>2]\n",
    "    return(len(temp))\n",
    "\n",
    "def words_to_dict(list_s):\n",
    "    wd = {}\n",
    "    #list_s = s.split(\" \")\n",
    "    for i in list_s:\n",
    "        if(len(i)>2):\n",
    "            if(i in wd.keys()):\n",
    "                wd[i] += 1\n",
    "            else:\n",
    "                wd[i] = 1\n",
    "    return wd\n",
    "\n",
    "def tf_calc(vocab, df, doc_len):\n",
    "    temp_list = []\n",
    "    for i in range(len(vocab.items())):\n",
    "        temp_list.append(0)\n",
    "    for item in vocab.keys():\n",
    "        if(item in df.keys()):\n",
    "            temp_list[vocab[item]] = df[item]/doc_len\n",
    "    return temp_list\n",
    "\n",
    "def ln(x):\n",
    "    n = 1000.0\n",
    "    return n * ((x ** (1/n)) - 1)\n",
    "\n",
    "def tfidf(idf, main_list2, vocab1, n):\n",
    "    temp_list = []\n",
    "    for i1 in range(len(vocab1.items())):\n",
    "        temp_list.append(0)\n",
    "    for i in range(len(temp_list3)):\n",
    "        temp_list[i] = idf[i] * main_list2[n][i]\n",
    "    return temp_list\n",
    "\n",
    "# FUNCTIONS END\n",
    "\n",
    "# Getting document length\n",
    "doc1_len = doc_len(s1_list)\n",
    "doc2_len = doc_len(s2_list)\n",
    "doc3_len = doc_len(s3_list)\n",
    "\n",
    "# Converting sentences to a dictionary bag of words\n",
    "df1 = words_to_dict(s1_list)\n",
    "df2 = words_to_dict(s2_list)\n",
    "df3 = words_to_dict(s3_list)\n",
    "\n",
    "# Creating a vocab of all the documents/samples\n",
    "all_words_list = list(set(df1.keys()).union(set(df2.keys())).union(set(df2.keys())))\n",
    "all_words_list.sort()\n",
    "vocab1 = {}\n",
    "vocab2 = {}\n",
    "for i in range(len(all_words_list)):\n",
    "    vocab1[all_words_list[i]] = i\n",
    "    vocab2[all_words_list[i]] = 0\n",
    "\n",
    "# Calculating the Term Frequencies\n",
    "main_list2.append(tf_calc(vocab1, df1, doc_len1))\n",
    "main_list2.append(tf_calc(vocab1, df2, doc_len2))\n",
    "main_list2.append(tf_calc(vocab1, df3, doc_len3))\n",
    "\n",
    "# Getting the number of sentences containing the word\n",
    "for i in vocab2.keys():\n",
    "    if i in df1.keys():\n",
    "        vocab2[i] += 1\n",
    "    if i in df2.keys():\n",
    "        vocab2[i] += 1\n",
    "    if i in df3.keys():\n",
    "        vocab2[i] += 1\n",
    "        \n",
    "# IDF Calc\n",
    "for j in vocab2.keys():\n",
    "    vocab2[j] = ln(3/vocab2[j])\n",
    "idf = list(vocab2.values())\n",
    "\n",
    "# Getting the TF-IDF Values\n",
    "tfidf = []\n",
    "tfidf.append(tfidf(idf, main_list2, vocab1, 0))\n",
    "tfidf.append(tfidf(idf, main_list2, vocab1, 1))\n",
    "tfidf.append(tfidf(idf, main_list2, vocab1, 2))\n",
    "#print(tfidf)\n",
    "\n",
    "# Doing cosine similarity for sample1 and sample2\n",
    "numer = 0\n",
    "for i in range(len(tfidf[0])):\n",
    "    numer += tfidf[0][i]*tfidf[1][i]\n",
    "\n",
    "den1 = 0\n",
    "for j in range(len(main_list[0])):\n",
    "    den1 += main_list[0][j] ** 2\n",
    "\n",
    "den2 = 0\n",
    "for k in range(len(main_list[1])):\n",
    "    den2 += main_list[1][k] ** 2\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABOVE TF-IDF CODE DID NOT HAVE GOOD OUTPUTS ON OUR CURRENT SAMPLE - NOT PART OF SOLUTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
